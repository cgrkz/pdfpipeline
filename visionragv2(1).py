# -*- coding: utf-8 -*-
"""visionragv2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/cgrkz04/visionragv2.50e14924-1a7a-4c00-aad3-6b167201f70c.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250610/auto/storage/goog4_request%26X-Goog-Date%3D20250610T182059Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Df52dc4ed66d0f5966b183de35785a0f83dd96187a84b9790602e10c6f3add0e7fe9fe0cc785d4daf15d692f3bd48f1854f5b1c2077d4f0b217365cf1e261a7feb309c6085343ba509baffa3321ecd33f36e3ce010a6fe5c1cc9da8a589b5e1038b1a8e9026e214bc1e30f7aa393b049563e3ef21c248aa2962b79c6cb65e4e6a322f4ef70cf51d066bfa6eab48f2af329d8b71869889fc4da449ad8e95976b22f2053c16d2787af3e1f79eec4fdcb270459cd92ff5c4247c46ab69e4aba0826ecf76f20b9a856badb01c855e0a3d1916c7fad39472abef0e804e6a038458b4240301115dbb67b752b115fb8e03945de675c0f6c0ea0d6dca0487c4644c0eb462
"""

!pip install -U -q byaldi pdf2image qwen-vl-utils transformers bitsandbytes peft

!pip install -U -q rerankers[monovlm]

!sudo apt-get install -y -q poppler-utils

import os
import shutil

input_dir = "/kaggle/input/visionrag1"
output_dir = "/kaggle/working/data"
os.makedirs(output_dir, exist_ok=True)

# Copy all uploaded PDFs to /kaggle/working/data/
for file in os.listdir(input_dir):
    if file.endswith(".pdf"):
        shutil.copy(os.path.join(input_dir, file), os.path.join(output_dir, file))

print("PDF files in data/:", os.listdir(output_dir))

import os
from pdf2image import convert_from_path

def convert_pdfs_to_images(pdf_folder):
    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(".pdf")]
    all_images = {}

    for doc_id, pdf_file in enumerate(pdf_files):
        pdf_path = os.path.join(pdf_folder, pdf_file)
        images = convert_from_path(pdf_path)
        all_images[doc_id] = images

    return all_images

all_images = convert_pdfs_to_images("/kaggle/working/data")

import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 8, figsize=(15, 10))

for i, ax in enumerate(axes.flat):
    img = all_images[0][i]
    ax.imshow(img)
    ax.axis("off")

plt.tight_layout()
plt.show()

from byaldi import RAGMultiModalModel

docs_retrieval_model = RAGMultiModalModel.from_pretrained("vidore/colqwen2-v1.0")

docs_retrieval_model.index(
    input_path="/kaggle/working/data",
    index_name="image_index",
    store_collection_with_index=False,
    overwrite=True
)

text_query = 'Using Figure 4.4, analyze the relationship between mitigation costs and potential, and explain why options costing USD 100 tCO2-eq⁻¹ or less could achieve significant emission reductions by 2030.'

results = docs_retrieval_model.search(text_query, k=3)
results

def get_grouped_images(results, all_images):
    grouped_images = []

    for result in results:
        doc_id = result["doc_id"]
        page_num = result["page_num"]
        grouped_images.append(
            all_images[doc_id][page_num - 1]
        )  # page_num are 1-indexed, while doc_ids are 0-indexed. Source https://github.com/AnswerDotAI/byaldi?tab=readme-ov-file#searching

    return grouped_images


grouped_images = get_grouped_images(results, all_images)

import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 3, figsize=(15, 10))

for i, ax in enumerate(axes.flat):
    img = grouped_images[i]
    ax.imshow(img)
    ax.axis("off")

plt.tight_layout()
plt.show()

from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor, BitsAndBytesConfig
from qwen_vl_utils import process_vision_info
import torch

# BitsAndBytesConfig int-4 config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16
)

# Load model and tokenizer
vl_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-VL-7B-Instruct", device_map="auto", torch_dtype=torch.bfloat16, quantization_config=bnb_config
)
vl_model.eval()

min_pixels = 224 * 224
max_pixels = 448 * 448
vl_model_processor = AutoProcessor.from_pretrained(
    "Qwen/Qwen2.5-VL-7B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels
)

chat_template = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": grouped_images[0],
            },
            {
                "type": "image",
                "image": grouped_images[1],
            },
            {
                "type": "image",
                "image": grouped_images[2],
            },
            {"type": "text", "text": text_query},
        ],
    }
]

text = vl_model_processor.apply_chat_template(chat_template, tokenize=False, add_generation_prompt=True)

image_inputs, _ = process_vision_info(chat_template)
inputs = vl_model_processor(
    text=[text],
    images=image_inputs,
    padding=True,
    return_tensors="pt",
)
inputs = inputs.to("cuda")

generated_ids = vl_model.generate(**inputs, max_new_tokens=500)

generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]
output_text = vl_model_processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)

print(output_text[0])

def answer_with_multimodal_rag(
    vl_model, docs_retrieval_model, vl_model_processor, grouped_images, text_query, top_k, max_new_tokens
):
    results = docs_retrieval_model.search(text_query, k=top_k)
    grouped_images = get_grouped_images(results, all_images)

    chat_template = [
        {
            "role": "user",
            "content": [{"type": "image", "image": image} for image in grouped_images]
            + [{"type": "text", "text": text_query}],
        }
    ]

    # Prepare the inputs
    text = vl_model_processor.apply_chat_template(chat_template, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(chat_template)
    inputs = vl_model_processor(
        text=[text],
        images=image_inputs,
        padding=True,
        return_tensors="pt",
    )
    inputs = inputs.to("cuda")

    # Generate text from the vl_model
    generated_ids = vl_model.generate(**inputs, max_new_tokens=max_new_tokens)
    generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]

    # Decode the generated text
    output_text = vl_model_processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )

    return output_text

output_text = answer_with_multimodal_rag(
    vl_model=vl_model,
    docs_retrieval_model=docs_retrieval_model,
    vl_model_processor=vl_model_processor,
    grouped_images=grouped_images,
    text_query="Using Figure 4.4, analyze the relationship between mitigation costs and potential, and explain why options costing USD 100 tCO2-eq⁻¹ or less could achieve significant emission reductions by 2030",
    top_k=3,
    max_new_tokens=500,
)
print(output_text[0])

